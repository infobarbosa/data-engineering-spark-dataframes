# Estruturas complexas (Arrays, Structs)

**Author:** Prof. Barbosa  
**Contact:** infobarbosa@gmail.com  
**Github:** [infobarbosa](https://github.com/infobarbosa)

---

#### Aten√ß√£o aos custos!
**Aten√ß√£o!** Ao realizar os laborat√≥rios deste m√≥dulo, lembre-se de que a execu√ß√£o na AWS pode gerar custos. A responsabilidade pela gest√£o desses custos √© do aluno.

---

## 1. Introdu√ß√£o
DataFrames no Spark podem conter estruturas de dados complexas como arrays e structs. Manipular esses tipos de dados requer t√©cnicas espec√≠ficas.

**Exemplo de c√≥digo:**
```python
### 1. Importe as bibliotecas necess√°rias:
from pyspark.sql import SparkSession
from pyspark.sql.functions import explode, col
from pyspark.sql.types import StringType, IntegerType, StructType, StructField, ArrayType

###  Inicialize o SparkSession:
spark = SparkSession.builder.appName("dataeng-complex-structures").getOrCreate()

### 3. Crie um DataFrame de exemplo:
data = [
    ("Jo√£o", [{"curso": "Matem√°tica", "nota": 85}, {"curso": "Hist√≥ria", "nota": 90}]),
    ("Maria", [{"curso": "Matem√°tica", "nota": 95}, {"curso": "Hist√≥ria", "nota": 80}])
]
schema = StructType([
    StructField("nome", StringType(), True),
    StructField("cursos", ArrayType(StructType([
        StructField("curso", StringType(), True),
        StructField("nota", IntegerType(), True)
    ])), True)
])
df = spark.createDataFrame(data, schema)

# Explodindo o array para linhas individuais
df_exploded = df.withColumn("curso", explode(df["cursos"]))
df_exploded.select("nome", col("curso.curso"), col("curso.nota")).show()

```

Ao trabalhar com arquivos JSON no PySpark, a l√≥gica de leitura segue alguns passos fundamentais que voc√™ precisa considerar para garantir uma leitura eficiente e correta. Aqui est√£o os principais pontos:

---

## 2. **Formato do Arquivo**
  - **Simples vs. Multilinha**: Um arquivo JSON pode ser escrito em um formato de linha √∫nica, onde cada linha √© um objeto JSON separado, ou em um formato multilinha, onde um objeto JSON pode se estender por v√°rias linhas. PySpark trata essas varia√ß√µes de maneira diferente.
  - **Formato padr√£o** (linhas √∫nicas): Cada linha deve conter um objeto JSON completo.
  - **Formato multilinha** (`multiLine=True`): O arquivo pode ter um objeto JSON distribu√≠do em v√°rias linhas.

---

## 3. **Estrutura do JSON**
  - **Estrutura plana**: Se o JSON cont√©m uma estrutura simples, onde os dados est√£o diretamente no n√≠vel superior (campos de chave-valor simples), a leitura ser√° direta.
  - **Estrutura aninhada**: Se o JSON cont√©m campos complexos (como listas, dicion√°rios aninhados), PySpark pode lidar com isso, mas voc√™ pode precisar "explodir" esses campos ou usar fun√ß√µes espec√≠ficas para manipular esses dados complexos.

---

## 4. **Schema Inference (Infer√™ncia de Esquema)**
  - Por padr√£o, PySpark tenta inferir automaticamente o esquema do JSON. No entanto, isso pode n√£o ser a abordagem mais eficiente ou precisa, especialmente para arquivos grandes ou complexos. Para evitar isso:
  - **Esquema expl√≠cito**: Voc√™ pode definir manualmente o esquema ao ler o arquivo para melhorar o desempenho e a precis√£o.
  - **Infer√™ncia de esquema autom√°tica**: Usar `inferSchema=True` √© uma op√ß√£o para JSONs simples, mas pode ser mais lento em arquivos muito grandes.

---

## 5. **Manuseio de Dados Complexos**
  **Array e objetos aninhados**: JSONs frequentemente cont√™m arrays ou objetos aninhados. Para manipular esses dados, voc√™ pode precisar usar fun√ß√µes como `explode()` para quebrar arrays ou acessar campos internos com `dot notation` (ex.: `dataframe.select("campo.objeto_interno")`).

#### üìå O que a fun√ß√£o explode faz?
A fun√ß√£o explode() transforma valores que est√£o em arrays (ou mapas) em v√°rias linhas, uma para cada elemento. √â usada quando voc√™ quer "desaninhar" estruturas complexas, como listas ou arrays de structs, para processar ou visualizar cada item separadamente.

#### ‚úÖ Quando √© necess√°rio usar explode?
Voc√™ deve usar explode quando:
- A coluna cont√©m listas ou arrays (ex: ArrayType)
- Voc√™ quer transformar cada item da lista em uma linha separada

No exemplo de c√≥digo apresentado anteriormente, manipulamos um DataFrame contendo uma coluna de arrays de structs (no caso, os cursos de cada aluno). Ao utilizar explode(df["cursos"]), transformamos cada elemento do array presente na coluna cursos em uma nova linha do DataFrame, mantendo as demais informa√ß√µes associadas ao registro original. Isso facilita a an√°lise e o processamento de dados aninhados, permitindo, por exemplo, visualizar cada curso e nota de um aluno em linhas separadas. Assim, o uso do explode √© fundamental para "desaninhar" estruturas complexas e trabalhar de forma mais eficiente com dados que possuem arrays ou listas em seu esquema.

---

## 6. **Leitura de Arquivos**
   O c√≥digo b√°sico para ler um arquivo JSON em PySpark √©:

   ```python
   from pyspark.sql import SparkSession

   # Criar a sess√£o do Spark
   spark = SparkSession.builder.appName("LeituraJSON").getOrCreate()

   # Ler o arquivo JSON
   df = spark.read.json("caminho_do_arquivo.json")

   # Mostrar os dados
   df.show()
   ```

   Se o arquivo JSON for multilinha, voc√™ deve especificar o par√¢metro `multiLine=True`:

   ```python
   df = spark.read.option("multiLine", True).json("caminho_do_arquivo.json")
   ```

---

## 7. **Considera√ß√µes de Desempenho**
  - **Particionamento**: Se o JSON for muito grande, considere o particionamento adequado para otimizar o processamento distribu√≠do no PySpark.
  - **Compress√£o**: Se o arquivo JSON estiver compactado (como `.gz` ou `.bz2`), PySpark pode ler diretamente esses arquivos sem descompact√°-los manualmente.
  - **Schema pr√©-definido**: Sempre que poss√≠vel, defina o esquema explicitamente para evitar infer√™ncias demoradas em grandes volumes de dados.

---

## 8. **Outro exemplo**
  Aqui est√° mais um exemplo completo com infer√™ncia de esquema e tratamento de um arquivo multilinha:

   ```python
   from pyspark.sql import SparkSession
   from pyspark.sql.types import StructType, StructField, StringType, IntegerType

   # Criar a sess√£o do Spark
   spark = SparkSession.builder.appName("LeituraJSON").getOrCreate()

   # Definir o esquema manualmente
   schema = StructType([
       StructField("id", IntegerType(), True),
       StructField("nome", StringType(), True),
       StructField("idade", IntegerType(), True),
       StructField("cidade", StringType(), True)
   ])

   # Ler o arquivo JSON com o esquema definido
   df = spark.read.option("multiline", "true").schema(schema).json("caminho_do_arquivo.json")

   # Mostrar os dados
   df.show()
   ```

---

## 9. Considera√ß√µes adicionais
  - **Tipo de dados**: Certifique-se de que os tipos de dados no esquema est√£o alinhados com os valores no JSON, especialmente em arquivos grandes.
  - **Tratamento de erros**: √Äs vezes, os arquivos JSON podem conter registros corrompidos ou malformados. Use `mode="DROPMALFORMED"` para ignorar esses registros.

---

## 10. Desafio (Arrays, Structs)

**Descri√ß√£o do Desafio:**

Voc√™ recebeu um dataset contendo informa√ß√µes de clientes de uma empresa. O dataset possui estruturas de dados complexas, como arrays e structs. Seu objetivo √© manipular esse dataset usando PySpark para extrair insights espec√≠ficos.

**Tarefas do desafio:**

1. **Flatten das Structs:**

  - Extraia as notas de cada mat√©ria (`matematica`, `portugues`, `ciencias`) e adicione como colunas separadas no DataFrame.

    Output esperado:
    ```
    +-----+-----+----------+---------+--------+
    |nome |idade|matematica|portugues|ciencias|
    +-----+-----+----------+---------+--------+
    |Ana  |25   |90        |85       |92      |
    |Bruno|30   |78        |88       |75      |
    |Carla|28   |85        |80       |88      |
    +-----+-----+----------+---------+--------+
    ```

2. **Explodir os Arrays de Contatos:**

  - Transforme o array de contatos em linhas individuais, de forma que cada contato (email ou telefone) fique em uma linha separada, mantendo o nome da pessoa associado.

    Output esperado:
    ```
    +-----+-----+--------+-----------------+
    |nome |idade|tipo    |valor            |
    +-----+-----+--------+-----------------+
    |Ana  |25   |email   |ana@example.com  |
    |Ana  |25   |telefone|123456789        |
    |Bruno|30   |email   |bruno@example.com|
    |Carla|28   |telefone|987654321        |
    +-----+-----+--------+-----------------+
    ```

3. **Explodir os Interesses:**

  - Fa√ßa o mesmo para o array de interesses, criando uma linha para cada interesse por pessoa.

    Output esperado:
    ```
    +-----+-----+---------+
    | nome|idade|interesse|
    +-----+-----+---------+
    |  Ana|   25|   m√∫sica|
    |  Ana|   25| esportes|
    |  Ana|   25|  leitura|
    |Bruno|   30|   cinema|
    |Bruno|   30|  viagens|
    |Carla|   28|     arte|
    |Carla|   28|  leitura|
    |Carla|   28|  viagens|
    +-----+-----+---------+
    ```  
---

**Dataset de Exemplo:**

O dataset est√° em formato JSON com o seguinte conte√∫do:

```json
[
  {
    "nome": "Ana",
    "idade": 25,
    "notas": {
      "matematica": 90,
      "portugues": 85,
      "ciencias": 92
    },
    "contatos": [
      {"tipo": "email", "valor": "ana@example.com"},
      {"tipo": "telefone", "valor": "123456789"}
    ],
    "interesses": ["m√∫sica", "esportes", "leitura"]
  },
  {
    "nome": "Bruno",
    "idade": 30,
    "notas": {
      "matematica": 78,
      "portugues": 88,
      "ciencias": 75
    },
    "contatos": [
      {"tipo": "email", "valor": "bruno@example.com"}
    ],
    "interesses": ["cinema", "viagens"]
  },
  {
    "nome": "Carla",
    "idade": 28,
    "notas": {
      "matematica": 85,
      "portugues": 80,
      "ciencias": 88
    },
    "contatos": [
      {"tipo": "telefone", "valor": "987654321"}
    ],
    "interesses": ["arte", "leitura", "viagens"]
  }
]
```

---

**C√≥digo Inicial:**

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import explode, col

# Iniciar uma sess√£o Spark
spark = SparkSession.builder.appName("DesafioPySpark").getOrCreate()

# Carregar o dataset JSON
caminho_arquivo = "caminho/para/o/dataset.json"
dados_clientes = spark.read.json(caminho_arquivo)

# Mostrar o schema do DataFrame
dados_clientes.printSchema()

# Mostrar os dados
dados_clientes.show(truncate=False)

# Escreva sua logica aqui

# Apresente o resultado aqui

```

##### Solu√ß√µes dos desafios

<details>
    <summary>Solu√ß√£o do Item 1: Flatten das Structs</summary>

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, coalesce

# Iniciar uma sess√£o Spark
spark = SparkSession.builder.appName("DesafioPySpark").getOrCreate()

# Carregar o dataset JSON
caminho_arquivo = "dataset.json"
dados_clientes = spark.read.json(caminho_arquivo, multiLine=True)

# Mostrar o schema do DataFrame original
dados_clientes.printSchema()

# Mostrar os dados originais
dados_clientes.show(truncate=False)

# ----------------------------------------------------------------------
# Solu√ß√£o do Item 1: Flatten das Structs
# ----------------------------------------------------------------------

# Importar as fun√ß√µes necess√°rias
from pyspark.sql.functions import col, coalesce

# Criar um novo DataFrame com as notas extra√≠das
dados_notas = dados_clientes \
    .withColumn("nota_matematica", col("notas.matematica")) \
    .withColumn("nota_portugues", col("notas.portugues")) \
    .withColumn("nota_ciencias", col("notas.ciencias"))

# Exibir o esquema atualizado do DataFrame
dados_notas.printSchema()

# Mostrar os dados com as novas colunas de notas
dados_notas.select("nome", "idade", "nota_matematica", "nota_portugues", "nota_ciencias").show()

```
</details>

<details>
    <summary>Solu√ß√£o do Item 2: Explodir os Arrays de Contatos</summary>

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import explode, col

# Iniciar uma sess√£o Spark
spark = SparkSession.builder.appName("DesafioPySpark").getOrCreate()

# Carregar o dataset JSON
caminho_arquivo = "dataset.json"
dados_clientes = spark.read.json(caminho_arquivo, multiLine=True)

# Mostrar o schema do DataFrame original
dados_clientes.printSchema()

# Mostrar os dados originais
dados_clientes.show(truncate=False)

# ----------------------------------------------------------------------
# Solu√ß√£o do Item 2: Explodir os Arrays de Contatos
# ----------------------------------------------------------------------

# Importar as fun√ß√µes necess√°rias
from pyspark.sql.functions import explode, col

# Explodir o array de contatos em linhas individuais
dados_contatos_explodido = dados_clientes.select(
    "nome",
    "idade",
    explode("contatos").alias("contato")
)

# Extrair os campos 'tipo' e 'valor' da struct 'contato'
dados_contatos_extracao = dados_contatos_explodido.select(
    "nome",
    "idade",
    col("contato.tipo").alias("tipo_contato"),
    col("contato.valor").alias("valor_contato")
)

# Exibir o esquema atualizado do DataFrame
dados_contatos_extracao.printSchema()

# Mostrar os dados ap√≥s a explos√£o dos contatos
dados_contatos_extracao.show(truncate=False)

```

**Resultado esperado**
```
root
 |-- nome: string (nullable = true)
 |-- idade: long (nullable = true)
 |-- tipo_contato: string (nullable = true)
 |-- valor_contato: string (nullable = true)

+-----+-----+------------+------------------+
|nome |idade|tipo_contato|valor_contato     |
+-----+-----+------------+------------------+
|Ana  |25   |email       |ana@example.com   |
|Ana  |25   |telefone    |123456789         |
|Bruno|30   |email       |bruno@example.com |
|Carla|28   |telefone    |987654321         |
+-----+-----+------------+------------------+

```
</details>

## 11. Parab√©ns!
Parab√©ns por concluir o m√≥dulo! Voc√™ aprendeu t√©cnicas manipula√ß√£o de DataFrames no Apache Spark explorando estruturas complexas em arrays e structs.

## 12. Destrui√ß√£o dos recursos
Para evitar custos desnecess√°rios, lembre-se de destruir os recursos criados durante este m√≥dulo:
- Exclua quaisquer inst√¢ncias do AWS Cloud9 que n√£o sejam mais necess√°rias.
- Remova dados tempor√°rios ou resultados intermedi√°rios armazenados no S3.

