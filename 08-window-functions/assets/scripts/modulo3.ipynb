{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data engineering programming\n",
    "\n",
    "**Author:** Prof. Barbosa  \n",
    "**Contact:** infobarbosa@gmail.com  \n",
    "**Github:** [infobarbosa](https://github.com/infobarbosa)\n",
    "\n",
    "\n",
    "Analise atentamente os comandos a seguir e execute-os em sequência.\n",
    "\n",
    "Caso você não tenha familiaridade com Jupyter notebooks, aqui está um mini tutorial para começar:\n",
    "\n",
    "Para executar células: Um notebook é composto por células. Cada célula pode conter código Python ou texto formatado. Para executar o código em uma célula, clique na célula para selecioná-la, depois pressione Shift+Enter. O resultado do código será exibido abaixo da célula.\n",
    "\n",
    "Novas células: Você pode adicionar novas células clicando no botão \"+\" na barra de ferramentas na parte superior. A nova célula será adicionada abaixo da célula atualmente selecionada.\n",
    "\n",
    "Salvar o notebook: Você pode salvar o notebook clicando no botão de Save na parte superior.\n",
    "\n",
    "Interromper o notebook: Se quiser sair, não esqueça de interromper o notebook utilizando o botão Stop notebook. Isso garante que os recursos alocados para o notebook sejam liberados e você não seja cobrado por isso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalação do PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast, row_number, rank, dense_rank, lag\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicializando a SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder.appName(\"dataeng-modulo-3\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando DataFrames de exemplo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame([(1, \"A\"), (2, \"B\"), (3, \"C\")], [\"id\", \"valor\"])\n",
    "df2 = spark.createDataFrame([(1, \"X\"), (2, \"Y\")], [\"id\", \"desc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_broadcast_join = df1.join(broadcast(df2), \"id\")\n",
    "df_broadcast_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle join\n",
    "\n",
    "\n",
    "\n",
    "O Shuffle Join é um algoritmo utilizado em processamento distribuído para combinar dois conjuntos de dados com base em uma chave comum. Ele é especialmente útil quando os dados estão distribuídos em diferentes nós de um cluster e precisam ser agrupados com base em uma chave compartilhada.\n",
    "\n",
    "Durante o Shuffle Join, os dados são particionados com base na chave de junção e redistribuídos entre os nós do cluster. Em seguida, os dados com a mesma chave são agrupados em cada nó e combinados para formar o resultado final.\n",
    "\n",
    "Esse algoritmo é eficiente para grandes volumes de dados, pois permite a paralelização do processamento e reduz a transferência de dados entre os nós. No entanto, ele pode exigir uma quantidade significativa de recursos computacionais e de rede, especialmente quando as chaves de junção são desbalanceadas.\n",
    "\n",
    "O Shuffle Join é amplamente utilizado em sistemas de processamento distribuído, como o Apache Hadoop e o Apache Spark, para realizar operações de junção em grandes conjuntos de dados distribuídos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_shuffle_join = df1.join(df2, \"id\")\n",
    "df_shuffle_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupBy com agregação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df1.groupBy(\"valor\").count()\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo uma janela de dados\n",
    "\n",
    "A classe pyspark.sql.window.Window fornece uma maneira de definir especificações de janela para agregações e funções analíticas no PySpark.\n",
    "\n",
    "Uma especificação de janela define a partição, ordenação e limites de quadro para uma função de janela. Isso permite que você faça cálculos em um subconjunto de linhas dentro de um DataFrame com base em critérios específicos.\n",
    "\n",
    "A classe Window é usada para criar instâncias de especificações de janela. Essas instâncias podem ser passadas para várias funções de janela no PySpark para realizar operações como classificação, agregações e cálculos baseados em janelas.\n",
    "\n",
    "Ao usar a classe Window, você pode facilmente definir especificações de janela complexas e aplicá-las aos seus dados para realizar análises avançadas e cálculos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "window_spec = Window.partitionBy(\"valor\").orderBy(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicando funções de janela\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_window = df1.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "df_window = df_window.withColumn(\"rank\", rank().over(window_spec))\n",
    "df_window = df_window.withColumn(\"dense_rank\", dense_rank().over(window_spec))\n",
    "df_window = df_window.withColumn(\"lag\", lag(\"id\", 1).over(window_spec))\n",
    "df_window.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encerrando a SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
